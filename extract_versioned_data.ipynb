{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "import yaml\n",
    "import requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a github access token is required for a large number of requests\n",
    "with open(\"config.yml\", 'r') as ymlfile:\n",
    "    cfg = yaml.safe_load(ymlfile)\n",
    "\n",
    "token = cfg['access_token']\n",
    "headers = {'Authorization': 'token ' + token}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_dict = {'deaths': 'time_series_covid19_deaths_US.csv',\n",
    "             'cases': 'time_series_covid19_confirmed_US.csv'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fips = pd.read_csv('locations.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download raw files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading files: deaths\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7af6f81376484421a41746554b92141a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/517 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading files: cases\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ae48100f48849bc88f061e0a7451023",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/517 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for target in file_dict:\n",
    "    # retrieve information about all commits that modified the file we want\n",
    "    all_commits = []\n",
    "\n",
    "    page = 0\n",
    "    while True:\n",
    "        page += 1\n",
    "        r = requests.get('https://api.github.com/repos/CSSEGISandData/COVID-19/commits',\n",
    "                         params = {'path': f'csse_covid_19_data/csse_covid_19_time_series/{file_dict[target]}',\n",
    "                                   'page': str(page)},\n",
    "                         headers = headers)\n",
    "        \n",
    "        if (not r.ok) or (r.text == '[]'):\n",
    "            break\n",
    "        \n",
    "        all_commits += json.loads(r.text or r.content)\n",
    "    \n",
    "    # dataframe of commit shas and corresponding commit dates\n",
    "    commits_df = pd.DataFrame({'date_time': [commit['commit']['author']['date'] for commit in all_commits],\n",
    "                               'sha': [entry['sha'] for entry in all_commits]})\n",
    "\n",
    "    commits_df.date_time = pd.to_datetime(commits_df.date_time)\n",
    "    commits_df['date'] = commits_df.date_time.dt.date\n",
    "    \n",
    "    # only consider last commit of each day\n",
    "    commits_df = commits_df.loc[commits_df.groupby('date')['date_time'].idxmax()]\n",
    "    \n",
    "    # download and save the csvs\n",
    "    print(f'Downloading files: {target}')\n",
    "    for _, row in tqdm(commits_df.iterrows(), total=commits_df.shape[0]):\n",
    "        result_path =  f'data/raw/{row.date}_JHU_raw_{target}.csv'\n",
    "        \n",
    "        # check if file already exists, don't save today's file as it might get updated again\n",
    "        if not os.path.isfile(result_path) and row.date != pd.Timestamp('today').date():\n",
    "            df = pd.read_csv(f'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/{row.sha}' \\\n",
    "                             f'/csse_covid_19_data/csse_covid_19_time_series/{file_dict[target]}')\n",
    "            df.to_csv(result_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(filepath, weekly=True):\n",
    "    df = pd.read_csv(filepath)\n",
    "    \n",
    "    # extract target from filepath\n",
    "    target = filepath.split('_')[-1][:-4]\n",
    "\n",
    "    df.drop(columns=['UID', 'iso2', 'iso3', 'code3', 'FIPS', 'Admin2', 'Country_Region', \n",
    "                     'Lat', 'Long_', 'Combined_Key', 'Population'], errors='ignore', inplace=True)\n",
    "\n",
    "    df = df.groupby('Province_State').sum().reset_index()\n",
    "    df = pd.melt(df, id_vars=['Province_State'])\n",
    "    df.columns = ['location_name', 'date', 'value']\n",
    "    df.date = pd.to_datetime(df.date)\n",
    "\n",
    "    df = df.merge(fips[['location', 'location_name']], how='left')\n",
    "    df = df[['date', 'location', 'location_name', 'value']].sort_values(['date', 'location'])\n",
    "    \n",
    "    if weekly:\n",
    "        df = df[df.date.dt.day_name() == 'Saturday'].reset_index(drop=True)\n",
    "    \n",
    "    df.drop(columns=['location_name'], inplace=True)\n",
    "\n",
    "    # compute national level\n",
    "    us = df.groupby('date')['value'].sum().reset_index()\n",
    "    us['location'] = 'US'\n",
    "    df.dropna(inplace=True) # drop Diamond Princess and Grand Princess (they are included in the national level)\n",
    "    df = pd.concat([df, us]).sort_values(['date', 'location']).reset_index(drop=True)\n",
    "\n",
    "    df.to_csv(f'data/cumulative_{target}/jhu_cumulative_{target}_as_of_{row.date.date()}.csv', index=False)\n",
    "\n",
    "    # compute incidence\n",
    "    df.value = df.groupby(['location'])['value'].diff()\n",
    "    df.dropna(inplace=True)\n",
    "    df.value = df.value.astype(int)\n",
    "\n",
    "    df.to_csv(f'data/incident_{target}/jhu_incident_{target}_as_of_{row.date.date()}.csv', index=False)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing files:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bc706cbeb2546acbd0e27c43ae19a1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/146 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "files = glob.glob('data/raw/*')\n",
    "\n",
    "# only consider data from Monday\n",
    "file_df = pd.DataFrame({'filepath': files})\n",
    "file_df['date'] = file_df.filepath.transform(lambda x: x.split('\\\\')[1][:10])\n",
    "file_df.date = pd.to_datetime(file_df.date)\n",
    "file_df = file_df[file_df.date.dt.day_name() == 'Monday'].reset_index(drop=True)\n",
    "\n",
    "print('Processing files:')\n",
    "for _, row in tqdm(file_df.iterrows(), total=file_df.shape[0]):\n",
    "    temp = process_file(row['filepath'], weekly=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
